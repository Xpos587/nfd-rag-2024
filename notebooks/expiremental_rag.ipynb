{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT-Structed_Output-Checklist-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q chromadb openai pydantic python-dotenv sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 21:07:35,877 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-10-10 21:07:35,901 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-10-10 21:07:35,920 - INFO - HTTP Request: GET http://localhost:8000/api/v1/tenants/default_tenant \"HTTP/1.1 200 OK\"\n",
      "2024-10-10 21:07:35,925 - INFO - HTTP Request: GET http://localhost:8000/api/v1/databases/default_database?tenant=default_tenant \"HTTP/1.1 200 OK\"\n",
      "/home/michael/Github/nfd-rag-2024/.micromamba/envs/default/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-10-10 21:07:38,545 - INFO - Load pretrained SentenceTransformer: cointegrated/LaBSE-en-ru\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://154.20.254.95:50856/v1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, List, Literal, Optional, Union\n",
    "\n",
    "import chromadb\n",
    "import orjson\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "VLLM_MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "VLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\")\n",
    "VLLM_API_KEY = os.getenv(\"VLLM_API_KEY\")\n",
    "CHROMA_HOST = os.getenv(\"CHROMA_HOST\")\n",
    "CHROMA_PORT = os.getenv(\"CHROMA_PORT\")\n",
    "CHROMA_COLLECTION_NAME = os.getenv(\"CHROMA_COLLECTION_NAME\")\n",
    "EMBEDDING_MODEL_NAME = os.getenv(\"EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "chroma_client = await chromadb.AsyncHttpClient(host=CHROMA_HOST, port=CHROMA_PORT)\n",
    "vllm_client = AsyncOpenAI(base_url=VLLM_BASE_URL, api_key=VLLM_API_KEY)\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "display(VLLM_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentAnalysis(BaseModel):\n",
    "    is_relevant: bool = Field(..., description=\"Релевантен ли документ запросу?\")\n",
    "    key_information: str = Field(..., description=\"Ключевая информация из документа\")\n",
    "\n",
    "class QueryAnalysis(BaseModel):\n",
    "    query_type: Literal[\"factual\", \"procedural\", \"conceptual\", \"other\"] = Field(..., description=\"Тип запроса\")\n",
    "    main_topic: str = Field(..., description=\"Основная тема запроса\")\n",
    "    required_info: List[str] = Field(..., description=\"Список необходимой информации для ответа\")\n",
    "\n",
    "class RetrievalStrategy(BaseModel):\n",
    "    query_type: Literal[\"semantic\", \"keyword\", \"hybrid\"] = Field(..., description=\"Тип поиска для извлечения документов\")\n",
    "    top_k: int = Field(..., description=\"Количество документов для извлечения\")\n",
    "    filter_criteria: Optional[Dict[str, Any]] = Field(None, description=\"Критерии фильтрации документов\")\n",
    "\n",
    "class AnswerFormulation(BaseModel):\n",
    "    main_points: List[str] = Field(..., description=\"Основные пункты ответа\")\n",
    "    additional_context: Optional[str] = Field(None, description=\"Дополнительный контекст\")\n",
    "    confidence_level: Literal[\"high\", \"medium\", \"low\"] = Field(..., description=\"Уровень уверенности в ответе\")\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    query_analysis: QueryAnalysis\n",
    "    document_analysis: List[DocumentAnalysis]\n",
    "    retrieval_strategy: RetrievalStrategy\n",
    "    answer_formulation: AnswerFormulation\n",
    "    final_answer: str\n",
    "    sources: List[str]\n",
    "\n",
    "async def retrieve_documents(query: str, strategy: RetrievalStrategy) -> List[dict]:\n",
    "    collection = await chroma_client.get_or_create_collection(\n",
    "        name=CHROMA_COLLECTION_NAME,\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "    results = await collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=strategy.top_k,\n",
    "        where=strategy.filter_criteria,\n",
    "    )\n",
    "    return [\n",
    "        {\"id\": id, \"content\": doc, \"metadata\": meta}\n",
    "        for id, doc, meta in zip(\n",
    "            results[\"ids\"][0], results[\"documents\"][0], results[\"metadatas\"][0]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "RESPONSE_GENERATION_PROMPT = '''Вопрос пользователя: {query}\n",
    "\n",
    "Контекст из документов:\n",
    "{context}\n",
    "\n",
    "Проанализируйте вопрос и контекст, затем сформулируйте ответ, следуя структуре:\n",
    "\n",
    "1. Анализ запроса\n",
    "2. Анализ документов\n",
    "3. Стратегия поиска\n",
    "4. Формулировка ответа\n",
    "5. Итоговый ответ\n",
    "6. Источники\n",
    "\n",
    "Ваш ответ должен быть в следующем формате JSON:\n",
    "\n",
    "{{\n",
    "    \"query_analysis\": {{\n",
    "        \"query_type\": \"factual\" | \"procedural\" | \"conceptual\" | \"other\",\n",
    "        \"main_topic\": \"<основная тема запроса>\",\n",
    "        \"required_info\": [\"<необходимая информация 1>\", \"<необходимая информация 2>\", ...]\n",
    "    }},\n",
    "    \"document_analysis\": [\n",
    "        {{\n",
    "            \"is_relevant\": true | false,\n",
    "            \"key_information\": \"<ключевая информация из документа>\"\n",
    "        }},\n",
    "        ...\n",
    "    ],\n",
    "    \"retrieval_strategy\": {{\n",
    "        \"query_type\": \"semantic\" | \"keyword\" | \"hybrid\",\n",
    "        \"top_k\": <число>,\n",
    "        \"filter_criteria\": null\n",
    "    }},\n",
    "    \"answer_formulation\": {{\n",
    "        \"main_points\": [\"<основной пункт 1>\", \"<основной пункт 2>\", ...],\n",
    "        \"additional_context\": \"<дополнительный контекст или null>\",\n",
    "        \"confidence_level\": \"high\" | \"medium\" | \"low\"\n",
    "    }},\n",
    "    \"final_answer\": \"<краткий и точный ответ на вопрос>\",\n",
    "    \"sources\": [\"<номер документа>\", ...]\n",
    "}}\n",
    "\n",
    "Пожалуйста, убедитесь, что ваш ответ строго соответствует этой структуре JSON.'''\n",
    "\n",
    "async def generate_response(query: str, documents: List[dict]) -> Union[ResponseModel, str]:\n",
    "    context = \"\\n\".join([f\"Document {i+1}: {doc['content']}\" for i, doc in enumerate(documents)])\n",
    "    prompt = RESPONSE_GENERATION_PROMPT.format(query=query, context=context)\n",
    "\n",
    "    try:\n",
    "        response = await vllm_client.chat.completions.create(\n",
    "            model=VLLM_MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "        response_data = orjson.loads(response_content)\n",
    "\n",
    "        # Проверяем наличие всех необходимых ключей\n",
    "        required_keys = [\"query_analysis\", \"document_analysis\", \"retrieval_strategy\", \"answer_formulation\", \"final_answer\", \"sources\"]\n",
    "        if not all(key in response_data for key in required_keys):\n",
    "            missing_keys = [key for key in required_keys if key not in response_data]\n",
    "            raise ValueError(f\"В ответе LLM отсутствуют следующие ключи: {', '.join(missing_keys)}\")\n",
    "\n",
    "        return ResponseModel(**response_data)\n",
    "    except orjson.JSONDecodeError as e:\n",
    "        logger.error(f\"Ошибка при разборе JSON: {e}\")\n",
    "        logger.error(f\"Содержимое ответа: {response_content}\")\n",
    "        return \"Ошибка при разборе ответа: неверный формат JSON\"\n",
    "    except ValidationError as e:\n",
    "        logger.error(f\"Ошибка валидации: {e}\")\n",
    "        return f\"Ошибка при валидации ответа: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Неожиданная ошибка: {e}\")\n",
    "        return f\"Произошла неожиданная ошибка: {str(e)}\"\n",
    "\n",
    "async def rag_assistant(query: str) -> str:\n",
    "    initial_strategy = RetrievalStrategy(query_type=\"semantic\", top_k=5)\n",
    "    documents = await retrieve_documents(query, initial_strategy)\n",
    "\n",
    "    response = await generate_response(query, documents)\n",
    "\n",
    "    if isinstance(response, str):\n",
    "        return response  # Возвращаем сообщение об ошибке\n",
    "\n",
    "    final_response = f\"\"\"\n",
    "    Анализ запроса:\n",
    "    - Тип запроса: {response.query_analysis.query_type}\n",
    "    - Основная тема: {response.query_analysis.main_topic}\n",
    "    - Требуемая информация: {', '.join(response.query_analysis.required_info)}\n",
    "\n",
    "    Анализ документов:\n",
    "    {' '.join([f\"Документ {i+1}: {'Релевантный' if doc.is_relevant else 'Нерелевантный'}\" for i, doc in enumerate(response.document_analysis)])}\n",
    "\n",
    "    Стратегия поиска:\n",
    "    - Тип поиска: {response.retrieval_strategy.query_type}\n",
    "    - Количество документов: {response.retrieval_strategy.top_k}\n",
    "    - Фильтры: {response.retrieval_strategy.filter_criteria}\n",
    "\n",
    "    Формулировка ответа:\n",
    "    - Основные пункты: {', '.join(response.answer_formulation.main_points)}\n",
    "    - Дополнительный контекст: {response.answer_formulation.additional_context}\n",
    "    - Уровень уверенности: {response.answer_formulation.confidence_level}\n",
    "\n",
    "    Итоговый ответ: {response.final_answer}\n",
    "\n",
    "    Источники информации: {', '.join(response.sources)}\n",
    "    \"\"\"\n",
    "\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 21:07:40,812 - INFO - HTTP Request: POST http://localhost:8000/api/v1/collections?tenant=default_tenant&database=default_database \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cd57a6de254ba4917dd0be9c830c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 21:07:40,934 - INFO - HTTP Request: POST http://localhost:8000/api/v1/collections/c6db97b8-8ea7-466b-81a1-eea76c40a6d6/query \"HTTP/1.1 200 OK\"\n",
      "2024-10-10 21:08:52,774 - INFO - HTTP Request: POST http://154.20.254.95:50856/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-10 21:08:52,778 - ERROR - Неожиданная ошибка: В ответе LLM отсутствуют следующие ключи: query_analysis, document_analysis, retrieval_strategy, answer_formulation, final_answer, sources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Произошла неожиданная ошибка: В ответе LLM отсутствуют следующие ключи: query_analysis, document_analysis, retrieval_strategy, answer_formulation, final_answer, sources\n"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "query = \"Какая тема занятия номер 2?\"\n",
    "answer = await rag_assistant(query)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
