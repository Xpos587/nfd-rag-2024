{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q chroma sentence_transformers ipywidgets pymupdf4llm pypandoc-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import uuid\n",
    "from typing import Dict, List\n",
    "\n",
    "import pymupdf4llm\n",
    "from chromadb import ClientAPI, HttpClient\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def office_to_pdf(file_path: str) -> str:\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    pdf_file = os.path.join(output_dir, os.path.splitext(base_name)[0] + \".pdf\")\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"libreoffice\",\n",
    "            \"--headless\",\n",
    "            \"--convert-to\",\n",
    "            \"pdf\",\n",
    "            file_path,\n",
    "            \"--outdir\",\n",
    "            output_dir,\n",
    "        ],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    return pdf_file\n",
    "\n",
    "\n",
    "def convert_to_markdown(file_path: str) -> Dict[str, str]:\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    metadata = {\n",
    "        \"original_file\": file_path,\n",
    "        \"file_type\": ext[1:],  # Убираем точку из расширения\n",
    "        \"conversion_method\": \"\",\n",
    "    }\n",
    "\n",
    "    converters = {\n",
    "        (\".doc\", \".docx\", \".rtf\"): lambda f: (\n",
    "            pymupdf4llm.to_markdown(\n",
    "                office_to_pdf(f),\n",
    "                write_images=False,\n",
    "                embed_images=False,\n",
    "                graphics_limit=None,\n",
    "                margins=(0, 0, 0, 0),\n",
    "                table_strategy=\"lines_strict\",\n",
    "                fontsize_limit=1,\n",
    "                ignore_code=True,\n",
    "                show_progress=False,\n",
    "            ),\n",
    "            \"office_to_pdf\",\n",
    "        ),\n",
    "        \".pdf\": lambda f: (\n",
    "            pymupdf4llm.to_markdown(\n",
    "                f,\n",
    "                write_images=False,\n",
    "                embed_images=False,\n",
    "                graphics_limit=None,\n",
    "                margins=(0, 0, 0, 0),\n",
    "                table_strategy=\"lines_strict\",\n",
    "                fontsize_limit=1,\n",
    "                ignore_code=True,\n",
    "                show_progress=False,\n",
    "            ),\n",
    "            \"direct_pdf\",\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    for extensions, converter in converters.items():\n",
    "        if ext in extensions:\n",
    "            content, method = converter(file_path)\n",
    "            metadata[\"conversion_method\"] = method\n",
    "            return {\"content\": content, \"metadata\": metadata}\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "def preprocess_markdown(markdown_content: str) -> str:\n",
    "    clean_content = markdown_content.lower()\n",
    "    clean_content = re.sub(r'<!--.*?-->', '', clean_content, flags=re.DOTALL)\n",
    "    clean_content = re.sub(r'[^\\w\\s.,;:?!-]', ' ', clean_content)\n",
    "    clean_content = re.sub(r'\\s+', ' ', clean_content).strip()\n",
    "    return clean_content\n",
    "\n",
    "def split_into_chunks(clean_content: str, chunk_size: int = 500) -> List[Dict[str, str]]:\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', clean_content)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) > chunk_size and current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"start_index\": clean_content.index(chunk_text),\n",
    "                \"end_index\": clean_content.index(chunk_text) + len(chunk_text)\n",
    "            })\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(sentence)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunk_text = ' '.join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"start_index\": clean_content.index(chunk_text),\n",
    "            \"end_index\": clean_content.index(chunk_text) + len(chunk_text)\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(chunks: List[Dict[str, str]], model_name: str = 'cointegrated/LaBSE-en-ru', batch_size: int = 8) -> List[List[float]]:\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for start in range(0, len(chunks), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_chunks = [chunk[\"text\"] for chunk in chunks[start:end]]\n",
    "        batch_embeddings = model.encode(batch_chunks, batch_size=batch_size, show_progress_bar=True)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def save_to_chroma(embeddings: List[List[float]], chunks: List[Dict[str, str]], metadata: Dict[str, str], chroma_client: ClientAPI):\n",
    "    collection = chroma_client.get_or_create_collection(\"my_collection\")\n",
    "    for embedding, chunk in zip(embeddings, chunks):\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        chunk_metadata = {\n",
    "            **metadata,\n",
    "            \"chunk_start\": chunk[\"start_index\"],\n",
    "            \"chunk_end\": chunk[\"end_index\"],\n",
    "            \"text\": chunk[\"text\"]\n",
    "        }\n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[chunk[\"text\"]],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[chunk_metadata]\n",
    "        )\n",
    "\n",
    "def process_document(file_path: str, chroma_client: ClientAPI):\n",
    "    conversion_result = convert_to_markdown(file_path)\n",
    "    markdown_content = conversion_result[\"content\"]\n",
    "    metadata = conversion_result[\"metadata\"]\n",
    "\n",
    "    clean_content = preprocess_markdown(markdown_content)\n",
    "    chunks = split_into_chunks(clean_content)\n",
    "    embeddings = create_embeddings(chunks)\n",
    "    save_to_chroma(embeddings, chunks, metadata, chroma_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = HttpClient(host='localhost', port=8000)\n",
    "file_paths = ['./data/public/Готовность ОПОП 2017.docx', './data/public/Карты_компетенций ЗФ китайский.doc']\n",
    "process_documents(file_paths, chroma_client)\n",
    "\n",
    "# Получение коллекции и просмотр количества документов\n",
    "collection = chroma_client.get_collection(\"my_collection\")\n",
    "results = collection.query(query_texts=[], n_results=5, include=[\"documents\", \"metadatas\"])\n",
    "logging.info(f\"Number of documents in collection: {len(results['ids'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
