{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q chroma sentence_transformers ipywidgets pymupdf4llm pypandoc-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import uuid\n",
    "from typing import Dict, List\n",
    "\n",
    "import pymupdf4llm\n",
    "import pypandoc\n",
    "from chromadb import ClientAPI, HttpClient\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def office_to_pdf(file_path: str) -> str:\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    pdf_file = os.path.join(output_dir, os.path.splitext(base_name)[0] + '.pdf')\n",
    "    subprocess.run(['libreoffice', '--headless', '--convert-to', 'pdf', file_path, '--outdir', output_dir], capture_output=True, text=True)\n",
    "    return pdf_file\n",
    "\n",
    "def excel_to_csv(file_path: str) -> str:\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    csv_file = os.path.join(output_dir, os.path.splitext(base_name)[0] + '.csv')\n",
    "    subprocess.run(['libreoffice', '--headless', '--convert-to', 'csv', file_path, '--outdir', output_dir], capture_output=True, text=True)\n",
    "    return csv_file\n",
    "\n",
    "def convert_to_markdown(file_path: str) -> Dict[str, str]:\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    metadata = {\n",
    "        \"original_file\": file_path,\n",
    "        \"file_type\": ext[1:],  # Убираем точку из расширения\n",
    "        \"conversion_method\": \"\"\n",
    "    }\n",
    "\n",
    "    converters = {\n",
    "        ('.doc', '.docx', '.rtf'): lambda f: (pymupdf4llm.to_markdown(office_to_pdf(f)), \"office_to_pdf\"),\n",
    "        '.pdf': lambda f: (pymupdf4llm.to_markdown(f), \"direct_pdf\"),\n",
    "        '.html': lambda f: (pypandoc.convert_file(f, 'md', format='html'), \"html_to_md\"),\n",
    "        ('.xls', '.xlsx'): lambda f: (pypandoc.convert_file(excel_to_csv(f), 'md', format='csv'), \"excel_to_csv_to_md\")\n",
    "    }\n",
    "\n",
    "    for extensions, converter in converters.items():\n",
    "        if ext in extensions:\n",
    "            content, method = converter(file_path)\n",
    "            metadata[\"conversion_method\"] = method\n",
    "            return {\"content\": content, \"metadata\": metadata}\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "def preprocess_markdown(markdown_content: str) -> str:\n",
    "    clean_content = markdown_content.lower()\n",
    "    clean_content = re.sub(r'<!--.*?-->', '', clean_content, flags=re.DOTALL)\n",
    "    clean_content = re.sub(r'[^\\w\\s.,;:?!-]', '', clean_content)\n",
    "    clean_content = re.sub(r'\\s+', ' ', clean_content).strip()\n",
    "    return clean_content\n",
    "\n",
    "def split_into_chunks(clean_content: str, chunk_size: int = 500) -> List[Dict[str, str]]:\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', clean_content)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) > chunk_size and current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"start_index\": clean_content.index(chunk_text),\n",
    "                \"end_index\": clean_content.index(chunk_text) + len(chunk_text)\n",
    "            })\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(sentence)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunk_text = ' '.join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"start_index\": clean_content.index(chunk_text),\n",
    "            \"end_index\": clean_content.index(chunk_text) + len(chunk_text)\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(chunks: List[Dict[str, str]], model_name: str = 'cointegrated/LaBSE-en-ru', batch_size: int = 8) -> List[List[float]]:\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for start in range(0, len(chunks), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_chunks = [chunk[\"text\"] for chunk in chunks[start:end]]\n",
    "        batch_embeddings = model.encode(batch_chunks, batch_size=batch_size, show_progress_bar=True)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def save_to_chroma(embeddings: List[List[float]], chunks: List[Dict[str, str]], metadata: Dict[str, str], chroma_client: ClientAPI):\n",
    "    collection = chroma_client.get_or_create_collection(\"my_collection\")\n",
    "    for embedding, chunk in zip(embeddings, chunks):\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        chunk_metadata = {\n",
    "            **metadata,\n",
    "            \"chunk_start\": chunk[\"start_index\"],\n",
    "            \"chunk_end\": chunk[\"end_index\"],\n",
    "            \"text\": chunk[\"text\"]\n",
    "        }\n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[chunk[\"text\"]],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[chunk_metadata]\n",
    "        )\n",
    "\n",
    "def process_document(file_path: str, chroma_client: ClientAPI):\n",
    "    conversion_result = convert_to_markdown(file_path)\n",
    "    markdown_content = conversion_result[\"content\"]\n",
    "    metadata = conversion_result[\"metadata\"]\n",
    "\n",
    "    clean_content = preprocess_markdown(markdown_content)\n",
    "    chunks = split_into_chunks(clean_content)\n",
    "    embeddings = create_embeddings(chunks)\n",
    "    save_to_chroma(embeddings, chunks, metadata, chroma_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./data/public/Готовность ОПОП 2017.pdf...\n",
      "[                                        ] (0/4=========[==========                              ] (1/4=========[====================                    ] (2/4=========[==============================          ] (3/4=========[========================================] (4/4]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0493cc15daa6462797d961739b79d8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chroma_client = HttpClient(host='localhost', port=8000)  # Настройте клиента Chroma\n",
    "file_path = './data/public/Готовность ОПОП 2017.docx'\n",
    "process_document(file_path, chroma_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение коллекции\n",
    "collection = chroma_client.get_collection(\"my_collection\")\n",
    "\n",
    "# Просмотр количества документов в коллекции\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
