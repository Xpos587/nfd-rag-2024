{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb openai python-dotenv pydantic sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Github/nfd-rag-2024/.micromamba/envs/default/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/michael/Github/nfd-rag-2024/.micromamba/envs/default/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x7dcce07cbf10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List, Literal, Optional\n",
    "\n",
    "from chromadb import AsyncHttpClient\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Model and settings\n",
    "QWEN_MODEL = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "vllm_client = AsyncOpenAI(\n",
    "    base_url=\"http://154.20.254.95:50856/v1\", api_key=\"dummy_key\"\n",
    ")\n",
    "\n",
    "# Initialize Sentence Transformer for embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "# Initialize Chroma\n",
    "chroma_client = await AsyncHttpClient(host=\"95.181.175.113\", port=8000)\n",
    "collection = await chroma_client.get_or_create_collection(\"rzd_documents\")\n",
    "\n",
    "# Models for structured output\n",
    "class DocumentReference(BaseModel):\n",
    "    title: str\n",
    "    section: str\n",
    "    relevance: Literal['high', 'medium', 'low']\n",
    "\n",
    "class ThinkStep(BaseModel):\n",
    "    reasoning: str\n",
    "    conclusion: str\n",
    "\n",
    "class Checklist(BaseModel):\n",
    "    direct_answer: bool = Field(..., description='Direct answer to the question provided')\n",
    "    document_reference: bool = Field(..., description='Reference to the relevant document')\n",
    "    additional_context: Optional[str] = Field(None, description='Additional context if necessary')\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    question_type: Literal['policy', 'procedure', 'safety', 'general', 'greeting']\n",
    "    relevant_documents: List[DocumentReference]\n",
    "    thinking_steps: List[ThinkStep]\n",
    "    final_answer: str\n",
    "    confidence: float = Field(..., ge=0, le=1)\n",
    "    checklist: Checklist\n",
    "\n",
    "# Functions for working with the knowledge base\n",
    "def create_embeddings(texts: List[str]):\n",
    "    return embedding_model.encode(texts)\n",
    "\n",
    "async def get_relevant_documents(query: str):\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    results = await collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()], n_results=5\n",
    "    )\n",
    "    return results[\"documents\"][0]\n",
    "\n",
    "def parse_response(response_text: str) -> Answer:\n",
    "    try:\n",
    "        # Попытка найти JSON в ответе\n",
    "        json_start = response_text.find('{')\n",
    "        json_end = response_text.rfind('}') + 1\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            json_str = response_text[json_start:json_end]\n",
    "            structured_output = json.loads(json_str)\n",
    "            textual_answer = response_text[json_end:].strip()\n",
    "        else:\n",
    "            # Если JSON не найден, считаем весь ответ текстовым\n",
    "            structured_output = {}\n",
    "            textual_answer = response_text.strip()\n",
    "\n",
    "        # Проверка и коррекция question_type\n",
    "        question_type = structured_output.get('question_type', 'general')\n",
    "        if question_type not in ['policy', 'procedure', 'safety', 'general', 'greeting']:\n",
    "            question_type = 'general'\n",
    "\n",
    "        # Заполнение структурированного вывода с чеклистом\n",
    "        checklist = Checklist(\n",
    "            direct_answer=structured_output.get('checklist', {}).get('direct_answer', False),\n",
    "            document_reference=structured_output.get('checklist', {}).get('document_reference', False),\n",
    "            additional_context=structured_output.get('checklist', {}).get('additional_context')\n",
    "        )\n",
    "\n",
    "        answer = Answer(\n",
    "            question_type=question_type,\n",
    "            relevant_documents=[DocumentReference(**doc) for doc in structured_output.get('relevant_documents', [])],\n",
    "            thinking_steps=[ThinkStep(**step) for step in structured_output.get('thinking_steps', [])],\n",
    "            final_answer=textual_answer or structured_output.get('final_answer', 'Ответ не предоставлен.'),\n",
    "            confidence=structured_output.get('confidence', 0.0),\n",
    "            checklist=checklist\n",
    "        )\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f'Ошибка при парсинге ответа: {str(e)}')\n",
    "        return Answer(\n",
    "            question_type='general',\n",
    "            relevant_documents=[],\n",
    "            thinking_steps=[],\n",
    "            final_answer=response_text or 'Не удалось получить структурированный ответ.',\n",
    "            confidence=0.0,\n",
    "            checklist=Checklist(direct_answer=False, document_reference=False)\n",
    "        )\n",
    "\n",
    "async def print_real_time_answer(answer: str):\n",
    "    for char in answer:\n",
    "        print(char, end=\"\", flush=True)\n",
    "        await asyncio.sleep(0.025)  # 25 ms delay\n",
    "    print()  # For line break at the end\n",
    "\n",
    "# Main function for asking a question\n",
    "async def ask_question(question: str):\n",
    "    system_prompt = (\n",
    "        \"You are an assistant answering questions for new employees of Russian Railways (RZD). \"\n",
    "        \"Use the provided knowledge base to answer. \"\n",
    "        \"If you cannot answer the question, indicate this with a `null` response in the structured output. \"\n",
    "        \"ALWAYS respond in Russian, regardless of the language of the question.\"\n",
    "    )\n",
    "\n",
    "    relevant_docs = await get_relevant_documents(question)\n",
    "\n",
    "    cot_prompt = (\n",
    "        \"Let's approach this step-by-step:\\n\"\n",
    "        \"1) Determine the type of employee query\\n\"\n",
    "        \"2) Find relevant information in the knowledge base\\n\"\n",
    "        \"3) Formulate a structured response\\n\\n\"\n",
    "    )\n",
    "\n",
    "    checklist_prompt = (\n",
    "        \"\\nEnsure your answer includes:\\n\"\n",
    "        \"1) A direct answer to the question\\n\"\n",
    "        \"2) Reference to the relevant document\\n\"\n",
    "        \"3) Additional context if necessary\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"{cot_prompt}\"\n",
    "        f\"QUESTION\\n\\n{question}\\n\\n\"\n",
    "        f\"KNOWLEDGE BASE\\n\\n{relevant_docs}\\n\\n\"\n",
    "        f\"{checklist_prompt}\\n\"\n",
    "        \"Provide your answer in the following format:\\n\"\n",
    "        \"1. A JSON object containing the structured information.\\n\"\n",
    "        \"2. A newline character.\\n\"\n",
    "        \"3. The textual answer in Russian.\\n\"\n",
    "        \"Example for a greeting:\\n\"\n",
    "        '{\"question_type\": \"greeting\", \"relevant_documents\": [], \"thinking_steps\": [], \"final_answer\": \"Здравствуйте!\", \"confidence\": 1.0, \"checklist\": {\"direct_answer\": true, \"document_reference\": false}}\\n'\n",
    "        \"Здравствуйте! Чем могу помочь?\\n\"\n",
    "        \"For other types of questions, use 'policy', 'procedure', 'safety', or 'general' as appropriate.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = await vllm_client.chat.completions.create(\n",
    "            model=QWEN_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "\n",
    "        parsed_response = parse_response(response.choices[0].message.content)\n",
    "\n",
    "        return parsed_response\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка при обработке запроса: {str(e)}\")\n",
    "        return Answer(\n",
    "            question_type='general',\n",
    "            relevant_documents=[],\n",
    "            thinking_steps=[],\n",
    "            final_answer=f\"Извините, произошла ошибка при обработке вашего запроса: {str(e)}\",\n",
    "            confidence=0.0,\n",
    "            checklist=Checklist(direct_answer=False, document_reference=False)\n",
    "        )\n",
    "\n",
    "# Function for processing a list of questions\n",
    "async def chat_loop():\n",
    "    print(\n",
    "        \"Чат-бот РЖД готов к работе. Введите ваш вопрос или 'выход' для завершения.\"\n",
    "    )\n",
    "    while True:\n",
    "        question = input(\"Ваш вопрос: \")\n",
    "        if question.lower() == \"выход\":\n",
    "            print(\"Выход из программы.\")\n",
    "            break\n",
    "        try:\n",
    "            response = await ask_question(question)\n",
    "\n",
    "            # Print structured output (for programmatic use)\n",
    "            print(\"\\nСтруктурированный вывод:\")\n",
    "            print(json.dumps(response.dict(), ensure_ascii=False, indent=2))\n",
    "\n",
    "            # Print text answer in real-time\n",
    "            print(\"\\nОтвет:\")\n",
    "            await print_real_time_answer(response.final_answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {str(e)}\")\n",
    "            print(\"Пожалуйста, попробуйте задать вопрос еще раз.\")\n",
    "\n",
    "# Run the chat loop\n",
    "await chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
